{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查询转换\n",
    "\n",
    "查询转换是一组专注于重写和/或修改问题以进行检索的方法。\n",
    "\n",
    "![Screenshot](./start.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境配置\n",
    "\n",
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据文档安装 `LangSmith` https://docs.smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langSmith 环境配置\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'your-api-key'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你是用 ` openAI` 作为 LLM，那么你需要在环境变量中设置 `OPENAI_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = <your-api-key>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我这里是用 azure 的服务，所以这里我的代码是这样子的，你可以根据自己的情况进行修改\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# 加载.env 文件\n",
    "dotenv.load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Query 多查询检索器\n",
    "\n",
    "基于距离的向量数据库检索通过将查询嵌入（表示）在高维空间中，并根据“距离”找到相似的嵌入文档。然而，检索结果可能会因查询措辞的细微变化或嵌入未能很好地捕捉数据语义而有所不同。为了解决这些问题，有时需要进行提示工程/调整，但这可能会很繁琐。\n",
    "\n",
    "MultiQueryRetriever 自动化了提示调整的过程，它使用大语言模型（LLM）从不同的角度为给定的用户输入查询生成多个查询。对于每个查询，它检索一组相关文档，并通过所有查询的唯一并集来获得更大范围的潜在相关文档集。通过为同一个问题生成多个角度的查询，MultiQueryRetriever 能够克服基于距离的检索的一些局限性，获取更加丰富的结果集。\n",
    "\n",
    "流程图如下:\n",
    "\n",
    "![Screenshot 2024-02-12 at 12.39.59 PM.png](./multi_query.png)\n",
    "\n",
    "代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文档\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://juejin.cn/post/7366149991159955466\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"article\", \"article-title\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "# 文档加载器加载文档\n",
    "docs = loader.load()\n",
    "\n",
    "# 文档拆分器\n",
    "# from_tiktoken_encoder 使用tiktoken编码器的文本分割器来计长度。\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# 文档拆分\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 实例化 embedding 模型 AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=os.environ.get(\"AZURE_EMBEDDING_TEXT_MODEL\")\n",
    ")\n",
    "\n",
    "# 构建向量索引\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建 prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 有哪些方法可以优化RAG？',\n",
       " '2. RAG的优化有哪些技巧可以使用？',\n",
       " '3. 如何提高RAG的性能？',\n",
       " '4. 有哪些策略可以改善RAG的效率？',\n",
       " '5. RAG优化方面有哪些值得尝试的方法？']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 是用 prompt 指令让 LLM 构建 5 个不同纬度的问题，来让 LLM 从向量数据库中检索相关文档。\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "# template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "# Generate multiple search queries related to: {question} \\n\n",
    "# Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 初始化 LLM， LLM 的配置会自动获取环境变量中的值。\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_DEPLOYMENT_NAME_GPT35\"),\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" 检索到文档的组合 \"\"\"\n",
    "    # 将列表展平，并将每个文档转换为字符串\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # 获取独特的文件\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # 返回 langchain documents\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "question = \"RAG优化技巧有哪些？\"\n",
    "result = generate_queries.invoke(question)\n",
    "# 这里可以看到 LLM 返回的 5 个问题。\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='查询转换\\n提高 RAG 系统效能的一个策略是添加一层查询理解层，也就是在实际进行检索前，先进行一系列的 Query Rewriting。具体而言，我们可以采用以下四种转换方法：\\n1.1 路由：在不改变原始查询的基础上，识别并导向相关的工具子集，并将这些工具确定为处理该查询的首选。'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='RAG优化技巧|7大挑战与解決方式|提高你的LLM能力\\n              \\n    demo007x\\n       \\n                    2024-05-08\\n                    \\n                    63\\n                   \\n                    阅读13分钟\\n                          RAG优化技巧|7大挑战与解決方式|提高你的LLM'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='1.调整检索策略\\nLangchain中提供许多检索的方法，确保我们在RAG中能拿到最符合问题的文件，详细的列表可以参考官网，其中包含：'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='通过对 RAG 系统挑战的深入分析和优化，我们不仅可以提升LLM的准确性和可靠性，还能大幅提高用户对技术的信任度和满意度。\\n希望这篇能帮助我们改善我们的 RAG 系统。\\n推荐阅读：\\n\\n\\n使用coze扣子搭建智能bot「程序员的工具箱」的思考和总结 - 掘金 (juejin.cn)\\n\\n\\nRAG实操教程: langchain+Milvus向量数据库创建你的本地知识库)'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='下图展示了RAG系统的两个主要流程：检索和查询；红色方框代表可能会遇到的挑战点，主要有7项：'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='1. 数据清洗\\n数据的质量直接影响到检索的效果，这个痛点再次突显了优质数据的重要性。在责备你的 RAG 系统之前，请确保你已经投入足够的精力去清洗数据。\\n2. 信息压缩'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='然而，尽管LLM + RAG的能力已经让人惊叹，但我们在使用RAG优化LLM的过程中，还是会遇到许多挑战和困难，包括但不限于检索器返回不准确或不相关的数据，并且基于错误或过时信息生成答案。因此本文旨在提出RAG常见的7大挑战，并附带各自相应的优化方案，期望能够帮助我们改善RAG。')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检索\n",
    "question = \"RAG优化技巧有哪些？\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "docs\n",
    "# len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLm 返回的 5 个不同纬度的问题的答案。\n",
    "这个流程首先将原始问题给到 LLM，然后 LLM 会返回 5 个不同纬度的问题，然后我们再将这 5 个问题给到 向量检索器，然后向量检索器会查找很多相关的答案，然后将这些 documents 进行唯一处理\n",
    "\n",
    "queyr -> LLM -> 5 queries -> Vector Store -> 5 documents -> LLM -> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建最后的 chatbot chain 应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG优化技巧包括添加一层查询理解层，进行一系列的Query Rewriting，以及采用四种转换方法：路由、数据清洗、信息压缩和对RAG系统挑战的深入分析和优化。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 构建最后的 RAG 链\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = final_rag_chain.invoke({\"question\": question})\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Fusion RAG 融合\n",
    "\n",
    "![rag-fusion](./fusion.png)\n",
    "\n",
    "RAG-Fusion 是一种搜索方法，旨在弥合传统搜索范式与人类查询多维度之间的差距。受检索增强生成（RAG）能力的启发，该项目更进一步，通过使用多重查询生成和互惠排名融合（Reciprocal Rank Fusion）对搜索结果进行重新排序。\n",
    "\n",
    "RAG-Fusion有两个特点：\n",
    "\n",
    "- 一个是问题增强，它会生成多个和原始问题类似的相关问题，提升检索的覆盖范围。举个例子，比如我想查询的是环境问题对我们的生活都会有什么影响，它可能就会帮你生成环境问题对我们的健康会有什么影响，环境问题对我们的经济会有什么，类似这样的相关问题。这样在向量搜索的时候，检索结果的覆盖面和多样性就更好了\n",
    "\n",
    "- 另外一个是RAG-Fusion使用了RRF 倒数排序融合作为排序方法，相比普通排序，RRF更依赖于每个排序中的相对排名，更擅长组合来自不同策略的查询结果，比如使用 BM25 和 Embedding 做多路召回，RRF 提供的重排质量确实要更好一点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion 这里要求 LLM 根据输入的问题产生多个问题\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 构建查询链\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='然而，尽管LLM + RAG的能力已经让人惊叹，但我们在使用RAG优化LLM的过程中，还是会遇到许多挑战和困难，包括但不限于检索器返回不准确或不相关的数据，并且基于错误或过时信息生成答案。因此本文旨在提出RAG常见的7大挑战，并附带各自相应的优化方案，期望能够帮助我们改善RAG。'),\n",
       "  0.06585580821434867),\n",
       " (Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='查询转换\\n提高 RAG 系统效能的一个策略是添加一层查询理解层，也就是在实际进行检索前，先进行一系列的 Query Rewriting。具体而言，我们可以采用以下四种转换方法：\\n1.1 路由：在不改变原始查询的基础上，识别并导向相关的工具子集，并将这些工具确定为处理该查询的首选。'),\n",
       "  0.06558258417063283),\n",
       " (Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='通过对 RAG 系统挑战的深入分析和优化，我们不仅可以提升LLM的准确性和可靠性，还能大幅提高用户对技术的信任度和满意度。\\n希望这篇能帮助我们改善我们的 RAG 系统。\\n推荐阅读：\\n\\n\\n使用coze扣子搭建智能bot「程序员的工具箱」的思考和总结 - 掘金 (juejin.cn)\\n\\n\\nRAG实操教程: langchain+Milvus向量数据库创建你的本地知识库)'),\n",
       "  0.048131080389144903),\n",
       " (Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='RAG优化技巧|7大挑战与解決方式|提高你的LLM能力\\n              \\n    demo007x\\n       \\n                    2024-05-08\\n                    \\n                    63\\n                   \\n                    阅读13分钟\\n                          RAG优化技巧|7大挑战与解決方式|提高你的LLM'),\n",
       "  0.032539682539682535),\n",
       " (Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='1. 数据清洗\\n数据的质量直接影响到检索的效果，这个痛点再次突显了优质数据的重要性。在责备你的 RAG 系统之前，请确保你已经投入足够的精力去清洗数据。\\n2. 信息压缩'),\n",
       "  0.032266458495966696),\n",
       " (Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='下图展示了RAG系统的两个主要流程：检索和查询；红色方框代表可能会遇到的挑战点，主要有7项：'),\n",
       "  0.015873015873015872)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" 将多个排名文档列表和在RRF公式中使用的可选参数k进行融合。\"\"\"\n",
    "\n",
    "    # 初始化一个字典，用于保存每个唯一文档的融合分数。\n",
    "    fused_scores = {}\n",
    "\n",
    "    # 遍历每个排名文档列表\n",
    "    for docs in results:\n",
    "        # 遍历列表中的每个文档，同时显示其排名（在列表中的位置）\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # 将文档转换为字符串格式以用作密钥（假设文档可以序列化为 JSON）。\n",
    "            doc_str = dumps(doc)\n",
    "            # 如果文档尚未在fused_scores字典中，将其添加并设置初始分数为0。\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # 如果有的话检索文档的当前分数\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # 使用 RRF 公式更新文档的分数：1 / (排名 + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # 根据它们的融合分数按降序对文档进行排序，以获取最终重新排名的结果。\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # 将重新排序的结果作为元组列表返回，每个元组包含文档和其融合得分。\n",
    "    return reranked_results\n",
    "\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "docs\n",
    "# len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG优化技巧包括添加一层查询理解层，进行一系列的Query Rewriting，路由，数据清洗和信息压缩。'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = final_rag_chain.invoke({\"question\": question})\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 什么是RAG优化技巧？', '2. RAG优化技巧的实际应用有哪些？', '3. 如何有效地实施RAG优化技巧？']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 构建 prompt，将一个问题分解为多个子问题\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 构建分解链\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# 测试分解链\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer recursively  递归回答\n",
    "\n",
    "![Screenshot 2024-02-18 at 1.55.32 PM.png](./answer_recursively.png)\n",
    "\n",
    "可能有的人会问，已经有了 `RAG-Fusion`，为什么还有 `Answer recursively` 递归回答呢？\n",
    "\n",
    "传统的文本生成方法通常涉及对模型的一次性查询，这可能会也可能不会产生所需的输出。迭代提示使我们能够以更可控的方式优化模型的输出。\n",
    "\n",
    "`RAG-Fusion` 的思想是将多个检索器的结果作为输入传递给模型，以生成更准确和多样化的输出。\n",
    "\n",
    "`Answer recursively`的思想是将模型的输出作为输入传递给模型，以生成更准确和多样化的输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 Answer recursively Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据提供的背景信息和相关问题+答案对，可以有效地实施RAG优化技巧的方法包括：\\n\\n1. 查询转换：通过添加查询理解层进行查询转换，识别并导向相关的工具子集，并将这些工具确定为处理查询的首选。\\n2. 数据清洗：确保投入足够的精力去清洗数据，因为数据的质量直接影响到检索的效果。\\n3. 信息压缩：采取信息压缩的方法来提高RAG系统的效能。\\n\\n这些方法旨在解决RAG系统面临的挑战和困难，以提高LLM的准确性和可靠性，以及提高用户对技术的信任度和满意度。因此，有效地实施RAG优化技巧需要在查询转换、数据清洗和信息压缩等方面进行综合考虑和实施。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"格式化问题和答案对\"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\" +  q_a_pair\n",
    "\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer individually 把问题拆分\n",
    "\n",
    "![Answer individually](./answer_individually.png)\n",
    "\n",
    "`Answer individually` 分开回答的思想是，将问题拆分成多个小问题，然后分别回答这些小问题，最后将这些小问题的答案组合起来一起让 LLM 生成，形成最终的答案。这种思想可以帮助我们更好地理解问题，也可以帮助我们更好地回答问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG优化技巧是指通过对RAG系统挑战的深入分析和优化，提升LLM的准确性和可靠性，以及提高用户对技术的信任度和满意度的方法和策略。\n",
      "RAG优化技巧的实际应用包括添加查询理解层，在实际进行检索前进行一系列的查询重写，以及进行数据清洗和信息压缩等方法。\n",
      "通过添加查询理解层，进行查询转换和数据清洗，以提高RAG系统的效能和准确性。同时，对RAG系统的挑战进行深入分析，并采取相应的优化方案，以提高用户对技术的信任度和满意度。\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1. 什么是RAG优化技巧？', '2. RAG优化技巧的实际应用有哪些？', '3. 如何有效地实施RAG优化技巧？']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "# prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt_template = \"\"\"Answer the following question with chinese based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt_rag = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"\n",
    "    对每个子问题进行检索和生成答案\n",
    "\n",
    "    参数：\n",
    "    question (str): 原始问题\n",
    "    prompt_rag (PromptTemplate): 用于生成答案的提示模板\n",
    "    sub_question_generator_chain (LLMChain): 用于生成子问题的链\n",
    "\n",
    "    返回：\n",
    "    rag_results (List[str]): 每个子问题的答案列表\n",
    "    sub_questions (List[str]): 生成的所有子问题列表\n",
    "    \"\"\"\n",
    "    # 使用我们的分解来生成子问题。\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "\n",
    "    # 初始化一个列表，用于保存 RAG 链的结果。\n",
    "    rag_results = []\n",
    "\n",
    "    # 遍历每个子问题\n",
    "    for sub_question in sub_questions:\n",
    "        # 检索与每个子问题相关的文件\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        # 使用RAG链中检索到的文件和子问题。\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs,\"question\": sub_question})\n",
    "        print(answer)\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "\n",
    "# 将检索和 RAG 过程封装在一个 RunnableLambda 中，以便集成到链中。\n",
    "answers, questions = retrieve_and_rag(\n",
    "    question, prompt_rag, generate_queries_decomposition)\n",
    "\n",
    "# 这里生成 3 个问题以及对应的答案。\n",
    "answers\n",
    "print(\"----------\")\n",
    "questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 qa_pairs 模式，将问题和答案作为一个整体进行检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG优化技巧包括通过对RAG系统挑战的深入分析和优化，提升LLM的准确性和可靠性，以及提高用户对技术的信任度和满意度的方法和策略。实际应用包括添加查询理解层，在实际进行检索前进行一系列的查询重写，以及进行数据清洗和信息压缩等方法。有效地实施RAG优化技巧需要通过添加查询理解层，进行查询转换和数据清洗，以提高RAG系统的效能和准确性，并对RAG系统的挑战进行深入分析，并采取相应的优化方案，以提高用户对技术的信任度和满意度。'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"格式化问题和答案对，以生成格式化的字符串\"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {\n",
    "            question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\": context,\"question\":question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Step Back 向后退\n",
    "\n",
    "![Screenshot 2024-02-12 at 1.14.43 PM.png](./step_back.png)\n",
    "\n",
    "`Step Back Prompting` 通过将复杂任务分解为抽象和推理两个步骤，帮助语言模型更好地处理包含大量细节的任务，提高其在复杂推理任务中的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# 我们现在将这些转换为示例消息。\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # 少样本提示词\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FewShotChatMessagePromptTemplate() 支持少量示例的聊天提示模板。\n",
    "\n",
    "该提示模板生成的高级结构是由前缀消息、示例消息和后缀消息组成的消息列表。\n",
    "\n",
    "这种结构使得创建具有中间示例对话变得可能，例如：\n",
    "\n",
    "系统：你是一个乐于助人的AI助手\n",
    "用户：2+2等于多少？\n",
    "AI：4\n",
    "用户：2+3等于多少？\n",
    "AI：5\n",
    "用户：4+4等于多少？\n",
    "\n",
    "这个提示模板可以用来生成固定列表的示例，也可以根据输入动态选择示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are some techniques for optimizing RAG?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "answer = generate_queries_step_back.invoke({\"question\": question})\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG优化技巧包括但不限于以下几种方法：\\n\\n1. 查询转换：添加一层查询理解层，即在实际进行检索前，先进行一系列的Query Rewriting。具体而言，可以采用路由、识别并导向相关的工具子集，并将这些工具确定为处理该查询的首选等转换方法。\\n\\n2. 数据清洗：数据的质量直接影响到检索的效果，因此在优化RAG系统时，需要确保已经投入足够的精力去清洗数据，以确保数据的质量和准确性。\\n\\n3. 信息压缩：在优化RAG系统时，可以考虑对信息进行压缩，以减少数据量和提高检索效率。\\n\\n4. 深入分析和优化：通过对RAG系统挑战的深入分析和优化，可以提升LLM的准确性和可靠性，同时提高用户对技术的信任度和满意度。\\n\\n5. 添加查询理解层：在实际进行检索前，先进行一系列的Query Rewriting，以提高RAG系统的效能和准确性。\\n\\n这些优化技巧可以帮助改善RAG系统的性能和效果，提高LLM的能力和可靠性。'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 响应提示\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # 使用正常问题获取上下文\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # 使用回溯问题来获取上下文\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # 把问题传给 LLM 回答\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = chain.invoke({\"question\": question})\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyDE\n",
    "\n",
    "![Screenshot 2024-02-12 at 1.12.45 PM.png](./hyde.png)\n",
    "\n",
    "\n",
    "HyDE（Hypothetical Document Embeddings）是一种用于零样本密集检索的方法，其工作原理如下：\n",
    "\n",
    "- 分解任务：将密集检索分解为两个任务，一个是由指令跟随语言模型执行的生成任务，另一个是由对比编码器执行的文档 - 文档相似性任务。\n",
    "- 生成假设文档：对于给定的查询，首先将其输入到指令跟随语言模型（如 InstructGPT）中，并指示它 “编写一个回答问题的文档”，即生成一个假设文档。该文档虽然不是真实的，可能包含事实错误，但期望它能够捕获相关性模式。\n",
    "- 编码假设文档：使用无监督对比编码器（如 Contriever）将假设文档编码为嵌入向量。在这里，期望编码器的密集瓶颈起到有损压缩机的作用，过滤掉嵌入向量中多余的（幻觉）细节。\n",
    "- 检索相似文档：使用这个向量在语料库嵌入中进行搜索，根据向量相似性检索最相似的真实文档并返回。检索利用了在对比训练期间编码在内积中的文档 - 文档相似性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG（Random Access Generator）是一种用于生成随机访问数据的技术，它可以用于优化数据访问的效率和性能。RAG优化技巧包括但不限于以下几点：\\n\\n1. 数据分布优化：通过对数据进行合理的分布和组织，可以减少数据访问的随机性，提高数据访问的效率。例如，可以将数据按照访问频率进行分组，将频繁访问的数据放置在更容易访问的位置，从而减少访问时间。\\n\\n2. 缓存优化：利用缓存技术可以减少数据访问的次数，提高数据访问的速度。RAG可以通过合理设置缓存大小和缓存策略来优化数据访问性能。\\n\\n3. 并行访问优化：利用并行访问技术可以同时处理多个数据访问请求，提高数据访问的并发性能。RAG可以通过合理设计并行访问的机制来优化数据访问的效率。\\n\\n4. 数据预取优化：通过预取数据可以减少数据访问的延迟，提高数据访问的速度。RAG可以通过预取数据的策略来优化数据访问性能。\\n\\n总之，RAG优化技巧可以通过合理的数据分布、缓存、并行访问和数据预取等手段来提高数据访问的效率和性能。这些技巧可以根据具体的应用场景和需求进行灵活的调整和优化，从而实现更高效的数据访问。'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE文档生成\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='然而，尽管LLM + RAG的能力已经让人惊叹，但我们在使用RAG优化LLM的过程中，还是会遇到许多挑战和困难，包括但不限于检索器返回不准确或不相关的数据，并且基于错误或过时信息生成答案。因此本文旨在提出RAG常见的7大挑战，并附带各自相应的优化方案，期望能够帮助我们改善RAG。'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='查询转换\\n提高 RAG 系统效能的一个策略是添加一层查询理解层，也就是在实际进行检索前，先进行一系列的 Query Rewriting。具体而言，我们可以采用以下四种转换方法：\\n1.1 路由：在不改变原始查询的基础上，识别并导向相关的工具子集，并将这些工具确定为处理该查询的首选。'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='1. 数据清洗\\n数据的质量直接影响到检索的效果，这个痛点再次突显了优质数据的重要性。在责备你的 RAG 系统之前，请确保你已经投入足够的精力去清洗数据。\\n2. 信息压缩'),\n",
       " Document(metadata={'source': 'https://juejin.cn/post/7366149991159955466'}, page_content='Auto Merging Retriever\\nMetadata Replacement + Node Sentence Window\\nRecursive Retriever\\n\\n总结\\n本文探讨了使用 RAG 技术时可能面临的七大挑战，并针对每个挑战提出了具体的优化方案，以提升系统准确性和用户体验。')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检索\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retireved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "retireved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG优化技巧包括添加查询理解层进行查询转换，数据清洗和信息压缩。'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建 RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer= final_rag_chain.invoke({\"context\": retireved_docs,\"question\":question})\n",
    "answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
